{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeoulGrandPark_Prediction_Kaggle.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbLW4VGMlqnEUY2GQP2L2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeGitaek/Kaggle_Solving/blob/master/SeoulGrandPark_Prediction_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk7sxNkO974V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "b2530cda-a5be-4194-f2ec-a6d1055f974d"
      },
      "source": [
        "!pip uninstall kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling kaggle-1.5.6:\n",
            "  Would remove:\n",
            "    /usr/local/bin/kaggle\n",
            "    /usr/local/lib/python3.6/dist-packages/kaggle-1.5.6.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/kaggle/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled kaggle-1.5.6\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.1.1\n",
            "Collecting kaggle==1.5.6\n",
            "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.4.5.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=f7941517f069e9a2c97be34e08c8d64588e4c4bb4a4e38fe03bff1ca933e97c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_hKsQHr-a0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2bbb655-a4c1-40d9-abb8-2bbaa93acb4a"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!ls -lha kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 68 Jun 19 09:57 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl201_90-cl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5669bf1a-3d3a-42e9-d902-12cc96c46fb8"
      },
      "source": [
        "!kaggle competitions download -c 2020-ai-termproject-18011817"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_seoul_grandpark.csv to /content\n",
            "  0% 0.00/1.14k [00:00<?, ?B/s]\n",
            "100% 1.14k/1.14k [00:00<00:00, 1.99MB/s]\n",
            "Downloading submit_sample.csv to /content\n",
            "  0% 0.00/466 [00:00<?, ?B/s]\n",
            "100% 466/466 [00:00<00:00, 390kB/s]\n",
            "Downloading train_seoul_grandpark.csv to /content\n",
            "  0% 0.00/24.3k [00:00<?, ?B/s]\n",
            "100% 24.3k/24.3k [00:00<00:00, 19.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQaEnCZb_k2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyTxbcPs_9Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "torch.manual_seed(777)\n",
        "random.seed(777)\n",
        "torch.cuda.manual_seed_all(777)\n",
        "\n",
        "learning_rate = 0.1\n",
        "training_epochs = 8000\n",
        "batch_size = 200\n",
        "drop_prob = 0.3"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtvJ2rjxAAos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy_train = pd.read_csv('train_seoul_grandpark.csv', header = None, skiprows=1, usecols=range(1, 8))\n",
        "\n",
        "x_data = xy_train.loc[: , 1:6]\n",
        "y_data = xy_train.loc[: , [7]]\n",
        "\n",
        "x_data = np.array(x_data)\n",
        "y_data = np.array(y_data)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "x_data = scaler.fit_transform(x_data)\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpQn6gxNACRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "data_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size, \n",
        "                                           shuffle = True, \n",
        "                                           drop_last = True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z3sGsOKAEQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear1 = torch.nn.Linear(6, 4,bias=True)\n",
        "linear2 = torch.nn.Linear(4, 4,bias=True)\n",
        "linear3 = torch.nn.Linear(4, 1,bias=True)\n",
        "#dropout = torch.nn.Dropout(p=drop_prob)\n",
        "relu = torch.nn.SELU()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jINDPXbeAGE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.nn.init.kaiming_normal_(linear1.weight)\n",
        "torch.nn.init.kaiming_normal_(linear2.weight)\n",
        "torch.nn.init.kaiming_normal_(linear3.weight)\n",
        "\n",
        "model = torch.nn.Sequential(linear1,relu,\n",
        "                            linear2,relu,\n",
        "                            linear3).to(device)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMVXorkNAJap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d27c5b59-58e2-46bf-de89-156f3a8d32c2"
      },
      "source": [
        "loss = torch.nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "losses = []\n",
        "model_history = []\n",
        "err_history = []\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "\n",
        "for epoch in range(training_epochs + 1):\n",
        "  avg_cost = 0\n",
        "  #model.train()\n",
        "  \n",
        "  for X, Y in data_loader:\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    cost = loss(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += cost / total_batch\n",
        "    \n",
        "  model_history.append(model)\n",
        "  err_history.append(avg_cost)\n",
        "  \n",
        "  if epoch % 100 == 0:  \n",
        "    print('Epoch:', '%d' % (epoch + 1), 'Cost =', '{:.9f}'.format(avg_cost))\n",
        "  losses.append(cost.item())\n",
        "print('Learning finished')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Cost = 106923528.000000000\n",
            "Epoch: 101 Cost = 51235000.000000000\n",
            "Epoch: 201 Cost = 43704484.000000000\n",
            "Epoch: 301 Cost = 41177308.000000000\n",
            "Epoch: 401 Cost = 37934600.000000000\n",
            "Epoch: 501 Cost = 40790416.000000000\n",
            "Epoch: 601 Cost = 38731360.000000000\n",
            "Epoch: 701 Cost = 40628800.000000000\n",
            "Epoch: 801 Cost = 39127492.000000000\n",
            "Epoch: 901 Cost = 39647432.000000000\n",
            "Epoch: 1001 Cost = 38923752.000000000\n",
            "Epoch: 1101 Cost = 40740036.000000000\n",
            "Epoch: 1201 Cost = 39594408.000000000\n",
            "Epoch: 1301 Cost = 37056172.000000000\n",
            "Epoch: 1401 Cost = 37864448.000000000\n",
            "Epoch: 1501 Cost = 40571464.000000000\n",
            "Epoch: 1601 Cost = 39973944.000000000\n",
            "Epoch: 1701 Cost = 40293052.000000000\n",
            "Epoch: 1801 Cost = 40287916.000000000\n",
            "Epoch: 1901 Cost = 37753632.000000000\n",
            "Epoch: 2001 Cost = 38713832.000000000\n",
            "Epoch: 2101 Cost = 39655280.000000000\n",
            "Epoch: 2201 Cost = 39118472.000000000\n",
            "Epoch: 2301 Cost = 39921032.000000000\n",
            "Epoch: 2401 Cost = 38843060.000000000\n",
            "Epoch: 2501 Cost = 38366184.000000000\n",
            "Epoch: 2601 Cost = 39548432.000000000\n",
            "Epoch: 2701 Cost = 39099928.000000000\n",
            "Epoch: 2801 Cost = 38711652.000000000\n",
            "Epoch: 2901 Cost = 39346548.000000000\n",
            "Epoch: 3001 Cost = 39489632.000000000\n",
            "Epoch: 3101 Cost = 38154384.000000000\n",
            "Epoch: 3201 Cost = 38389468.000000000\n",
            "Epoch: 3301 Cost = 39305756.000000000\n",
            "Epoch: 3401 Cost = 39943780.000000000\n",
            "Epoch: 3501 Cost = 39278168.000000000\n",
            "Epoch: 3601 Cost = 39241680.000000000\n",
            "Epoch: 3701 Cost = 38013080.000000000\n",
            "Epoch: 3801 Cost = 38832628.000000000\n",
            "Epoch: 3901 Cost = 38614808.000000000\n",
            "Epoch: 4001 Cost = 38712648.000000000\n",
            "Epoch: 4101 Cost = 38499916.000000000\n",
            "Epoch: 4201 Cost = 38797524.000000000\n",
            "Epoch: 4301 Cost = 37002832.000000000\n",
            "Epoch: 4401 Cost = 38457840.000000000\n",
            "Epoch: 4501 Cost = 37771320.000000000\n",
            "Epoch: 4601 Cost = 38653968.000000000\n",
            "Epoch: 4701 Cost = 39407916.000000000\n",
            "Epoch: 4801 Cost = 39557780.000000000\n",
            "Epoch: 4901 Cost = 37415192.000000000\n",
            "Epoch: 5001 Cost = 38276272.000000000\n",
            "Epoch: 5101 Cost = 39066640.000000000\n",
            "Epoch: 5201 Cost = 39171780.000000000\n",
            "Epoch: 5301 Cost = 38968532.000000000\n",
            "Epoch: 5401 Cost = 39620200.000000000\n",
            "Epoch: 5501 Cost = 38442612.000000000\n",
            "Epoch: 5601 Cost = 37806336.000000000\n",
            "Epoch: 5701 Cost = 36617752.000000000\n",
            "Epoch: 5801 Cost = 38663256.000000000\n",
            "Epoch: 5901 Cost = 38661256.000000000\n",
            "Epoch: 6001 Cost = 39632968.000000000\n",
            "Epoch: 6101 Cost = 38661268.000000000\n",
            "Epoch: 6201 Cost = 39359648.000000000\n",
            "Epoch: 6301 Cost = 39800496.000000000\n",
            "Epoch: 6401 Cost = 38443764.000000000\n",
            "Epoch: 6501 Cost = 38304000.000000000\n",
            "Epoch: 6601 Cost = 36988032.000000000\n",
            "Epoch: 6701 Cost = 37904728.000000000\n",
            "Epoch: 6801 Cost = 38142488.000000000\n",
            "Epoch: 6901 Cost = 38467064.000000000\n",
            "Epoch: 7001 Cost = 38910860.000000000\n",
            "Epoch: 7101 Cost = 39842660.000000000\n",
            "Epoch: 7201 Cost = 35834168.000000000\n",
            "Epoch: 7301 Cost = 38545084.000000000\n",
            "Epoch: 7401 Cost = 37946496.000000000\n",
            "Epoch: 7501 Cost = 38687640.000000000\n",
            "Epoch: 7601 Cost = 38665992.000000000\n",
            "Epoch: 7701 Cost = 38448228.000000000\n",
            "Epoch: 7801 Cost = 38960248.000000000\n",
            "Epoch: 7901 Cost = 38238632.000000000\n",
            "Epoch: 8001 Cost = 38304452.000000000\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcnl0HQiAMMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model = model_history[np.argmin(err_history)]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EeYK3yFAOBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy_test = pd.read_csv('test_seoul_grandpark.csv', header = None, skiprows=1, usecols = range(1, 7))\n",
        "x_data = xy_test.loc[:, 1:6]\n",
        "x_data = np.array(x_data)\n",
        "x_data = scaler.transform(x_data)\n",
        "x_test = torch.FloatTensor(x_data).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()     \n",
        "    predict = best_model(x_test)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKpHUf59ARiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46153ee7-06db-49a5-8f95-ab74672e1659"
      },
      "source": [
        "submit = pd.read_csv('submit_sample.csv')\n",
        "submit['Expected'] = submit['Expected'].astype(float)\n",
        "for i in range(len(predict)):\n",
        "  submit['Expected'][i] = predict[i]\n",
        "submit.to_csv('submit.csv', mode = 'w', index = False, header = True)\n",
        "submit"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Expected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20170330</td>\n",
              "      <td>8388.352539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20180418</td>\n",
              "      <td>10198.683594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20190430</td>\n",
              "      <td>12329.935547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20180618</td>\n",
              "      <td>10470.603516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20160329</td>\n",
              "      <td>9991.739258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20170424</td>\n",
              "      <td>11660.111328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>20180719</td>\n",
              "      <td>2527.253662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20170828</td>\n",
              "      <td>7479.009277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20180227</td>\n",
              "      <td>1390.254395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20160211</td>\n",
              "      <td>5202.645996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>20170511</td>\n",
              "      <td>14679.666016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>20190301</td>\n",
              "      <td>4504.360352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20161126</td>\n",
              "      <td>2387.242920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20160505</td>\n",
              "      <td>15071.302734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20160727</td>\n",
              "      <td>2108.822754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20180913</td>\n",
              "      <td>9818.379883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>20170714</td>\n",
              "      <td>2221.893799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>20180607</td>\n",
              "      <td>10297.968750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>20171218</td>\n",
              "      <td>1807.456177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20171111</td>\n",
              "      <td>5068.199219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20190125</td>\n",
              "      <td>-33.584343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20170621</td>\n",
              "      <td>8695.283203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>20170929</td>\n",
              "      <td>14290.792969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>20181224</td>\n",
              "      <td>2222.179932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>20181103</td>\n",
              "      <td>9974.792969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>20170611</td>\n",
              "      <td>18229.228516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>20180917</td>\n",
              "      <td>10548.302734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>20160430</td>\n",
              "      <td>11810.494141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>20170512</td>\n",
              "      <td>12375.718750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>20190217</td>\n",
              "      <td>2110.718018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id      Expected\n",
              "0   20170330   8388.352539\n",
              "1   20180418  10198.683594\n",
              "2   20190430  12329.935547\n",
              "3   20180618  10470.603516\n",
              "4   20160329   9991.739258\n",
              "5   20170424  11660.111328\n",
              "6   20180719   2527.253662\n",
              "7   20170828   7479.009277\n",
              "8   20180227   1390.254395\n",
              "9   20160211   5202.645996\n",
              "10  20170511  14679.666016\n",
              "11  20190301   4504.360352\n",
              "12  20161126   2387.242920\n",
              "13  20160505  15071.302734\n",
              "14  20160727   2108.822754\n",
              "15  20180913   9818.379883\n",
              "16  20170714   2221.893799\n",
              "17  20180607  10297.968750\n",
              "18  20171218   1807.456177\n",
              "19  20171111   5068.199219\n",
              "20  20190125    -33.584343\n",
              "21  20170621   8695.283203\n",
              "22  20170929  14290.792969\n",
              "23  20181224   2222.179932\n",
              "24  20181103   9974.792969\n",
              "25  20170611  18229.228516\n",
              "26  20180917  10548.302734\n",
              "27  20160430  11810.494141\n",
              "28  20170512  12375.718750\n",
              "29  20190217   2110.718018"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85qi7gzmBbNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a25defee-1a6b-4458-f39d-6dad8cbb086b"
      },
      "source": [
        "!kaggle competitions submit -c 2020-ai-termproject-18011817 -f submit.csv -m \"14010974_이기택\""
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 767/767 [00:10<00:00, 74.4B/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/cli.py\", line 64, in main\n",
            "    print(out, end='')\n",
            "UnicodeEncodeError: 'latin-1' codec can't encode characters in position 26-30: ordinal not in range(256)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}